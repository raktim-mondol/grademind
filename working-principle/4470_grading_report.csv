Student,Total Marks,Overall Feedback,Task 1.1 Marks,Task 1.2 Marks,Task 1.3 Marks,Task 2.1 Marks,Task 2.2 Marks,Task 3.1 Marks,Task 3.2.1 Marks,Task 3.2.2 Marks,Task 3.2.3 Marks,Task 3.3.1 Marks,Task 3.3.2 Marks,Task 3.3.3 Marks,Task 3.3.4 Marks,Task 3.3.5 Marks,Task 3.4.1 Marks,Task 3.4.2 Marks,Task 3.4.3 Marks,Task 3.5.1 Marks,Task 3.5.2 Marks,Task 3.5.3 Marks
2806832817 - Martin Peng - 2744791_Martin_Peng_z5580411_8279939_2111805061.ipynb,23,Task 3.2.1 (-2.0): (-2.0) The plots showing accuracy and loss changes on the test set as hyperparameters vary for the tree-based models were not provided.,1,0.5,0.5,1,2,2,0,1,1,1,1,2,2,1,1,2,1,1,1,1
2807262803 - Ronan Dsouza - 2729822_Ronan_Dsouza_z5565382-Ronan-Dsouza_8279939_832561930.ipynb,21.5,"Task 1.2 (-0.5): (-0.5) Concatenating train and test sets before applying get_dummies causes data leakage. Categories that appear only in the test set are used to inform the encoding of the training set. | Task 3.2.1 (-0.2): (-0.2) The plot for the Bagging model has an incorrect title ('Random Forest Complexity...'). It appears to be a copy-paste error. | Task 3.5.1 (-0.8): (-0.8) Critical bug in the experiment code. The loop variable 'Dh' was not used to define the number of neurons in the hidden layer; a fixed value 'hidden_units' was used instead. Consequently, the same network was trained in every iteration, and the experiment to test different neuron counts was not actually performed. Partial marks awarded for the correct loop and plot generation structure. | Task 3.5.2 (-1.0): (-1.0) The explanation of overfitting is generic and not based on observations from the provided plots, as the experiment was flawed due to the bug in task 3.5.1. The plots do not show the behavior described. | Task 3.5.3 (-1.0): (-1.0) The conclusion about the optimal number of neurons is invalid. It is based on the flawed experiment from 3.5.1, where all networks tested had the identical architecture.",1,0,0.5,1,2,2,1.8,1,1,1,1,2,2,1,1,2,1,0.2,0,0
2807595304 - Chong Liu - 2704106_Chong_Liu_Assignment2-Notebook_8279939_1621330469.ipynb,22,"Task 2.2 (-0.5): (-0.5) The AdaBoostClassifier was implemented with a RandomForestClassifier as the base estimator. The requirement was to use Decision Stumps (i.e., DecisionTreeClassifier(max_depth=1)). | Task 3.3.3 (-1.0): (-1.0) The performance table with metrics was provided, but the required confusion matrices for the baseline models were not. | Task 3.3.4 (-1.0): (-1.0) A valid technique (class weighting) was applied and results were reported in a table, but the required plot comparing performance before and after handling the imbalance is missing. | Task 3.3.5 (-0.5): (-0.5) The response identifies the best-performing model from the results but does not explain the underlying reasons why that model might be more robust to class imbalance (e.g., how ensemble methods might help).",1,0.5,0.5,1,1.5,2,2,1,1,1,1,1,1,0.5,1,2,1,1,1,1
2808091219 - Yian Zhu - 2680858_Yian_Zhu_assignment2_8279939_843079499.ipynb,25,Well done. Excellent work.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
2809697184 - Boyang Zhang - 2764095_Boyang_Zhang_z5598846_BoyangZhang_8279939_1799013120.ipynb,17,"Task 3.2.1 (-2.0): (-2.0) Plots showing model performance vs. hyperparameter changes are completely missing. | Task 3.3.2 (-0.5): (-0.5) The code was not tested on all three datasets as required by the rubric; only 'creditcard' and 'adult' were shown. | Task 3.3.4 (-1.0): (-1.0) A valid technique (class_weight) was applied, but the required plot comparing performance before and after the fix was not provided. Only the 'after' performance was plotted. | Task 3.3.5 (-1.0): (-1.0) The discussion on which models handle class imbalance better is completely missing. | Task 3.4.1 (-1.0): (-1.0) The code to randomly flip 20% of training labels is missing. | Task 3.4.2 (-2.0): (-2.0) Since the label flipping code was not implemented, the models were not retrained on noisy data, and no comparison was performed. | Task 3.5.2 (-0.5): (-0.5) The student correctly described what overfitting looks like but failed to identify the clear signs of overfitting (diverging train/test loss and accuracy) in their own plots for higher neuron counts.",1,0.5,0.5,1,2,2,0,1,1,1,0.5,2,1,0,0,0,1,1,0.5,1
2810531363 - Xinhao Luo - 2660417_Xinhao_Luo_ass2_8279939_1849233277.ipynb,23,"Task 3.3.3 (-0.5): (-0.5) The rubric required reporting performance using confusion matrices in addition to other metrics, but the confusion matrices were not provided for the initial performance on the imbalanced dataset. | Task 3.5.2 (-1.0): (-1.0) Incorrect analysis. The plot clearly shows signs of overfitting as the number of neurons increases: a growing gap appears between the training and test accuracy curves, which you failed to identify. | Task 3.5.3 (-0.5): (-0.5) The optimal number of neurons was correctly identified from the plot, but the discussion of the rule of thumb was insufficient. You did not state the rule or perform the calculation to support your claim that your result was consistent with it.",1,0.5,0.5,1,2,2,2,1,1,1,1,1.5,2,1,1,2,1,1,0,0.5
2810656221 - Xincheng Hao - 2762746_Xincheng_Hao_z5597528-Xincheng-Hao.ipynb_8279939_766676484.ipynb,24.5,"Task 3.5.3 (-0.5): (-0.5) The optimal number of neurons and the rule-of-thumb value were correctly identified and plotted, but the explicit written discussion comparing them for consistency was missing.",1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,0.5
2810874407 - Zhicheng Hu - 2672387_Zhicheng_Hu_z5512539_Zhicheng_Hu_8279939_216028464.ipynb,23.5,"Task 3.2.3 (-0.5): (-0.5) Analysis of overfitting is incomplete. It correctly identifies overfitting in AdaBoost but fails to discuss the other models as requested, which also show clear signs of overfitting (e.g., large train-test accuracy gaps). | Task 3.3.2 (-0.5): (-0.5) Incorrectly identified 'Covertype' as the highly imbalanced dataset to proceed with, when 'Creditcard' is significantly more imbalanced. The testing loop contained a 'break' statement that caused it to stop after finding the first imbalanced dataset, preventing a proper comparison. | Task 3.4.3 (-0.5): (-0.5) Correctly identified the most robust model based on the experimental results but did not provide the required explanation for why that model might have higher resistance to noise.",1,0.5,0.5,1,2,2,2,1,0.5,1,0.5,2,2,1,1,2,0.5,1,1,1
2811624956 - Mingrui Lin - 2713091_Mingrui_Lin_z5547873_Mingrui_Lin_8279939_1348126344.ipynb,20.75,"Task 1.2 (-0.2): (-0.25) Data leakage introduced by concatenating train and test sets before applying pd.get_dummies. The encoder should be defined using only the training data. | Task 3.3.4 (-1.0): (-1.0) The handling technique was applied correctly and results were reported in a table, but the required plot comparing performance before and after is missing. | Task 3.5.1 (-1.0): (-1.0) Task not attempted. No code or plots were provided for tuning the number of neurons. | Task 3.5.2 (-1.0): (-1.0) Task not attempted. Analysis of overfitting requires the plots from the previous task, which were not provided. | Task 3.5.3 (-1.0): (-1.0) Task not attempted. Determining the optimal number of neurons requires the results from the previous task, which were not provided.",1,0.25,0.5,1,2,2,2,1,1,1,1,2,1,1,1,2,1,0,0,0
2812059810 - Divya Tyagi - 2675337_Divya_Tyagi_Ass2_8279939_346575046.ipynb,25,Well done. Excellent work.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
2813267662 - Charlie Griffith - 2797989_Charlie_Griffith_Charlie_Griffith_Assignment_2_COMP9414_8279939_602675060.ipynb,23,Task 3.3.4 (-1.0): (-1.0) Missing plot comparing performance of original vs weighted models. Only tables were provided. | Task 3.4.2 (-1.0): (-1.0) Missing plots comparing performance on clean vs. noisy data. Only tables were provided for comparison.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,1,1,1,1,1,1,1,1
2813490470 - Haodong Wei - 2662945_Haodong_Wei_z5503884_ass2_8279939_1254268675.ipynb,24,"Task 2.1 (-0.5): (-0.5) The model implementation is correct, but the learning curve plot requested in the task description is missing from the notebook output. | Task 3.3.5 (-0.5): (-0.5) The discussion is logical, but the presented plots and tables are for the 'CoverType' dataset, while the text discusses the 'creditcard' dataset. This contradicts the evidence presented.",1,0.5,0.5,0.5,2,2,2,1,1,1,1,2,2,0.5,1,2,1,1,1,1
2813497077 - Ruizhe Wang - 2760255_Ruizhe_Wang_assignment2_8279939_1987046667.ipynb,20.5,"Task 1.2 (-0.5): (-0.5) The code concatenates training and testing sets before applying `pd.get_dummies`. This allows the encoder to learn about categories present in the test set, which constitutes data leakage. | Task 3.2.1 (-2.0): (-2.0) The required plots showing accuracy and loss changes on the test set for each model (DT, RF, Bagging, AdaBoost) as hyperparameters vary are missing from the report. | Task 3.2.2 (-0.5): (-0.5) The explanation of how accuracies change is theoretically correct, but it is not supported by empirical results from the student's own experiments, as the required plots were not generated. | Task 3.2.3 (-0.5): (-0.5) The discussion on overfitting is sound in theory but lacks the connection to the student's own experimental results, which should have been presented in the missing plots. | Task 3.3.4 (-0.5): (-0.5) The code correctly applies class weighting and resampling techniques and reports the results in tables. However, the rubric explicitly asked for a plot comparing the performance, which was not provided. | Task 3.5.3 (-0.5): (-0.5) The optimal number of neurons was correctly identified based on the experiments. However, the analysis failed to discuss whether this number was consistent with the rule of thumb, as requested by the question.",1,0,0.5,1,2,2,0,0.5,0.5,1,1,2,1.5,1,1,2,1,1,1,0.5
2813809883 - Xianze Li - 2792068_Xianze_Li_z5623960-Xianze-Li-Assignment_2_8279939_1228520783.ipynb,23.75,"Task 2.1 (-1.0): (-1.0) The implementation for `train_shallow_net_class` is missing. | Task 3.3.3 (-0.2): (-0.25) The performance table is good, but it omits the confusion matrix which was explicitly asked for in the rubric.",1,0.5,0.5,0,2,2,2,1,1,1,1,1.75,2,1,1,2,1,1,1,1
2813826280 - Haikun Ping - 2784382_Haikun_Ping_z5617152-haikun-ping_8279939_1420150411.ipynb,16.5,"Task 1.2 (-0.5): (-0.5) Concatenating train and test sets before one-hot encoding leads to data leakage, as information about categories present only in the test set is used to define the feature space for the training set. | Task 3.1 (-0.5): (-0.5) The explanations are brief and incomplete sentences. While the core ideas are present, the explanation for why tree-based models are invariant to scaling could be more detailed (e.g., mentioning that splits are based on feature thresholds, so only the ordering of values matters). | Task 3.2.2 (-1.0): (-1.0) The analysis is an incomplete sentence and does not explain how both training and test accuracies change with model complexity. It fails to discuss the typical trend of test accuracy (increasing then potentially decreasing). | Task 3.2.3 (-1.0): (-1.0) The answer is an incomplete sentence. It correctly identifies models that overfit but provides no explanation or reasoning based on the generated plots. | Task 3.3.4 (-1.0): (-1.0) A plot comparing the performance before and after applying class weighting is missing as required by the rubric. Only tables are provided. | Task 3.3.5 (-1.0): (-1.0) The discussion is an incomplete sentence and fails to identify which model handles imbalance better or provide any reasoning. | Task 3.4.3 (-0.5): (-0.5) The answer correctly identifies AdaBoost as more robust based on the plots, but the explanation is an incomplete sentence and not specific enough to justify the choice. | Task 3.5.1 (-1.0): (-1.0) The required plots showing training/test accuracies and losses while modifying the number of neurons are missing. | Task 3.5.2 (-1.0): (-1.0) The analysis of overfitting refers to plots that were not included in the submission. Without the plots, the observations cannot be verified. | Task 3.5.3 (-1.0): (-1.0) The determination of the optimal number of neurons is based on experimental results (plots) that are missing from the submission. The explanation is also an incomplete sentence.",1,0,0.5,1,2,1.5,2,0,0,1,1,2,1,0,1,2,0.5,0,0,0
2814117500 - Songning Liu - 2842901_Songning_Liu_Assignment_2_-_notebook_-_z5668551_8279939_1983350368.ipynb,25,Well done. Excellent work.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
2814207541 - Chen Wang - 2798430_Chen_Wang_z5629478-Chen-WANG_8279939_1484591340.ipynb,23.5,"Task 1.2 (-0.5): (-0.5) Concatenating train and test sets before one-hot encoding causes data leakage. Information about categories present only in the test set is leaked to the training process. | Task 3.2.1 (-0.5): (-0.5) Plots for Bagging, Random Forest, and AdaBoost incorrectly label the x-axis as 'Max Depth' when the hyperparameter being varied is 'n_estimators'. | Task 3.2.3 (-0.5): (-0.5) The analysis for Bagging and Random Forest is incorrect. A large, stable gap between train accuracy (near 1.0) and test accuracy is a clear sign of overfitting, even if test accuracy doesn't decrease.",1,0,0.5,1,2,2,1.5,1,0.5,1,1,2,2,1,1,2,1,1,1,1
2814279060 - Anson Cheng - 2500771_Anson_Cheng_Assignment_2_8279939_341818222.ipynb,25,Well done. Excellent work.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
2815207042 - Michael Pham - 45218_Michael_Pham_z3332615_-_Michael_Pham_8279939_946738274.ipynb,25,Well done. Excellent work.,1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
2815699014 - Zijun Tao - 2774578_Zijun_Tao_z5608581-zijun-tao_8279939_1033184466.ipynb,19.25,"Task 1.2 (-0.5): (-0.5) Combined train and test sets before one-hot encoding. This causes data leakage, as categories present only in the test set are used to define the feature space for the training set. The encoder should be determined using only the training data. | Task 2.1 (-0.8): (-0.75) The function incorrectly processes the input y_train, which is already one-hot encoded. The code `np.ravel(y_train)` followed by `np.unique` results in `num_classes` being incorrectly calculated as 2 for any dataset. This causes the code to crash on multi-class datasets like 'covertype' due to a shape mismatch between model output and labels. | Task 3.1 (-1.0): (-1.0) The answer identifies the correct high-level concepts but lacks detailed explanation. It does not fully explain *why* tree-based models are invariant (i.e., splits are based on ordering, which scaling preserves) or fully detail the consequences for NNs (e.g., skewed cost function contours leading to slow convergence). The provided text was also incomplete. | Task 3.3.4 (-0.5): (-0.5) Excellent implementation and reporting of both class weighting and SMOTE. However, the required plot comparing the performance before and after applying the balancing technique is missing. | Task 3.4.2 (-1.0): (-1.0) The implementation of the comparison methodology (tables and plots) is correct. However, the analysis was performed on the 'creditcard' dataset instead of the 'adult income' dataset as specified in the rubric. | Task 3.4.3 (-0.5): (-0.5) The analysis correctly interprets the provided plots, but the reasoning is superficial. The evaluation is also based on the wrong dataset as noted in the previous task. | Task 3.5.2 (-0.5): (-0.5) The provided explanation is a correct textbook definition of overfitting. However, it is not linked to the actual plots generated, which do not show clear signs of overfitting. The analysis should be based on the student's own experimental results. | Task 3.5.3 (-1.0): (-1.0) The answer is incomplete and does not address the question. It does not state the optimal number of neurons based on the experiment, and it completely omits the required discussion of whether this result is consistent with the rule-of-thumb calculation.",1,0,0.5,0.25,2,1,2,1,1,1,1,2,1.5,1,1,1,0.5,1,0.5,0
2816471996 - Hanchao Xie - 2660068_Hanchao_Xie_Assignment2_8279939_1050553429.ipynb,24.5,"Task 3.3.5 (-0.5): (-0.5) The analysis is too brief. It notes that AdaBoost was significantly affected by weighting but does not explain which model handles imbalance better or why (e.g., by discussing the mechanisms of bagging/boosting).",1,0.5,0.5,1,2,2,2,1,1,1,1,2,2,0.5,1,2,1,1,1,1
2819240738 - Junji Duan - 2808647_Junji_Duan_z5638051_Junji_Duan_8279939_1054363625.ipynb,24.5,"Task 1.2 (-0.5): (-0.5) Concatenating train and test sets before applying get_dummies causes data leakage. The set of all possible categories is learned from the test set, which is information that should not be available during training.",1,0,0.5,1,2,2,2,1,1,1,1,2,2,1,1,2,1,1,1,1
